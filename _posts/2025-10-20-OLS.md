---
title: "Ordinary Least Squares: Assumptions and Statistical Properties"
layout: single
math: true
excerpt: "A rigorous derivation of the OLS estimator, its unbiasedness, consistency, and asymptotic normality under classical linear model assumptions."
---

## 1. Model Setup

We consider the standard linear regression model in matrix form:

$$
y = X\beta + \varepsilon,
$$

where  

- \(y \in \mathbb{R}^{n \times 1}\) is the vector of dependent variables,  
- \(X \in \mathbb{R}^{n \times k}\) is the matrix of regressors (full column rank \(k\)),  
- \(\beta \in \mathbb{R}^{k \times 1}\) is the vector of unknown parameters,  
- \(\varepsilon \in \mathbb{R}^{n \times 1}\) is the vector of disturbances.

---

## 2. Classical Assumptions (Gauss–Markov)

1. **Linearity**  
   \(y = X\beta + \varepsilon\).

2. **Full Rank**  
   \(\operatorname{rank}(X) = k\), ensuring \(X'X\) is invertible.

3. **Exogeneity (Zero-mean errors)**  
   \(\mathbb{E}[\varepsilon | X] = 0\).

4. **Homoskedasticity and No Serial Correlation**  
   \(\operatorname{Var}(\varepsilon | X) = \sigma^2 I_n\).

5. **Finite variance**  
   \(\mathbb{E}[\varepsilon_i^2] < \infty\).

6. (**For asymptotics**)  
   As \(n \to \infty\),
   \[
   \frac{1}{n} X'X \xrightarrow{p} Q,
   \quad Q \text{ positive definite.}
   \]

---

## 3. Derivation of the OLS Estimator

The OLS estimator minimizes the residual sum of squares:

$$
\hat{\beta}
  = \arg\min_b (y - Xb)'(y - Xb).
$$

Differentiate w.r.t. \(b\):

$$
\frac{\partial}{\partial b}(y - Xb)'(y - Xb)
= -2X'(y - Xb) = 0.
$$

Solving:

$$
\boxed{\hat{\beta} = (X'X)^{-1} X'y.}
$$

Substitute \(y = X\beta + \varepsilon\):

$$
\hat{\beta} = \beta + (X'X)^{-1} X'\varepsilon.
$$

---

## 4. Unbiasedness of OLS

Taking expectations conditional on \(X\):

$$
\mathbb{E}[\hat{\beta} | X]
= \beta + (X'X)^{-1} X'\mathbb{E}[\varepsilon | X]
= \beta.
$$

Hence

$$
\boxed{\mathbb{E}[\hat{\beta}] = \beta,}
$$

so the OLS estimator is **unbiased**.

---

## 5. Variance of the OLS Estimator

Given \(\operatorname{Var}(\varepsilon | X) = \sigma^2 I_n\):

\[
\operatorname{Var}(\hat{\beta} | X)
= (X'X)^{-1} X' \operatorname{Var}(\varepsilon | X) X (X'X)^{-1}
= \sigma^2 (X'X)^{-1}.
\]

Thus the sampling variability of \(\hat{\beta}\) depends on sample size and multicollinearity.

---

## 6. Consistency of OLS

From
\[
\hat{\beta} - \beta = (X'X)^{-1} X'\varepsilon
= \left( \frac{1}{n} X'X \right)^{-1} \frac{1}{n} X'\varepsilon.
\]

If  

\[
\frac{1}{n} X'X \xrightarrow{p} Q, \quad
\frac{1}{n} X'\varepsilon \xrightarrow{p} 0,
\]
then by Slutsky’s theorem,

\[
\boxed{\hat{\beta} \xrightarrow{p} \beta.}
\]

Hence OLS is **consistent** under weak exogeneity and finite second moments.

---

## 7. Asymptotic Normality

We can rewrite

\[
\sqrt{n}(\hat{\beta} - \beta)
= \left( \frac{X'X}{n} \right)^{-1}
  \frac{X'\varepsilon}{\sqrt{n}}.
\]

By the Lindeberg–Feller Central Limit Theorem,

\[
\frac{1}{\sqrt{n}} X'\varepsilon
\xrightarrow{d} \mathcal{N}(0, \sigma^2 Q),
\]
where \( Q = \lim_{n\to\infty} \frac{1}{n} X'X.\)

Applying Slutsky’s theorem again:

\[
\boxed{
\sqrt{n}(\hat{\beta} - \beta)
\xrightarrow{d}
\mathcal{N}(0, \sigma^2 Q^{-1}).
}
\]

Thus the OLS estimator is **asymptotically normal** with asymptotic variance \( \sigma^2 Q^{-1} \).

---

## 8. Summary

| Property | Expression | Key Conditions |
|-----------|-------------|----------------|
| Estimator | \( \hat{\beta} = (X'X)^{-1}X'y \) | \(X'X\) invertible |
| Unbiasedness | \( \mathbb{E}[\hat{\beta}] = \beta \) | \( \mathbb{E}[\varepsilon|X]=0 \) |
| Variance | \( \operatorname{Var}(\hat{\beta}) = \sigma^2(X'X)^{-1} \) | Homoskedasticity |
| Consistency | \( \hat{\beta}\xrightarrow{p}\beta \) | LLN, finite moments |
| Asymptotic Normality | \( \sqrt{n}(\hat{\beta}-\beta)\xrightarrow{d}\mathcal{N}(0,\sigma^2Q^{-1}) \) | CLT, exogeneity |

---
